{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2A0v-mxcq2X0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "W0-mzjT7q-_m"
      },
      "outputs": [],
      "source": [
        "def is_same(im1, im2, eps=0.01):\n",
        "    return np.sum(np.abs(im1 - im2) / np.prod(im1.shape)) <= eps\n",
        "\n",
        "def print_res_match(im1, im2, eps=0.01):\n",
        "    print('Result Match = ' + str(is_same(im1, im2, eps)))\n",
        "\n",
        "def print_shape_match(im1, im2):\n",
        "    print('Shape Match = ' + str(im1.shape == im2.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "M8WtOv28rEd7"
      },
      "outputs": [],
      "source": [
        "# import the reference code (master solution)\n",
        "# and the student's code\n",
        "student = __import__('pylayer')# if len(sys.argv) == 1 else sys.argv[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fVehk-TNrIcx"
      },
      "outputs": [],
      "source": [
        "print('Testing Conv2d:')\n",
        "# initialize Conv2d\n",
        "student_conv = student.Conv2d(3, 6, kernel_size=3, stride=2, padding=1)\n",
        "ref_conv = nn.Conv2d(3, 6, kernel_size=3, stride=2, padding=1, bias=True)\n",
        "# align input data\n",
        "conv_data_pt = Variable(torch.rand(1, 3, 32, 32), requires_grad = True)\n",
        "conv_data_numpy = conv_data_pt.data.numpy()\n",
        "# align weight and bias\n",
        "ref_conv.weight.data = torch.from_numpy(student_conv.weight).clone().detach().requires_grad_(True)\n",
        "ref_conv.bias.data = torch.from_numpy(student_conv.bias).clone().detach().requires_grad_(True)\n",
        "# check output\n",
        "ref_out = ref_conv(conv_data_pt)\n",
        "student_out = student_conv.forward(conv_data_numpy)\n",
        "print_shape_match(ref_out.data.numpy(), student_out)\n",
        "print_res_match(ref_out.data.numpy(), student_out)\n",
        "# check gradients\n",
        "grad_out, grad_w, grad_b = student_conv.backward(np.ones_like(student_out))\n",
        "ref_out.backward(torch.ones_like(ref_out))\n",
        "print_res_match(conv_data_pt.grad.data.numpy(), grad_out)\n",
        "print_res_match(ref_conv.weight.grad.data.numpy(), grad_w)\n",
        "print_res_match(ref_conv.bias.grad.data.numpy(), grad_b)\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tkHBQ7_DrSN2"
      },
      "outputs": [],
      "source": [
        "print('Testing Linear:')\n",
        "# initialize Linear\n",
        "student_linear = student.Linear(30, 10)\n",
        "ref_linear = nn.Linear(30, 10, bias=True)\n",
        "# align input data\n",
        "linear_data_pt = Variable(torch.rand(1, 30), requires_grad = True)\n",
        "linear_data_numpy = linear_data_pt.data.numpy()\n",
        "# align weight and bias\n",
        "# NOTE: Use transpose is because the slight shape definition difference between PyNet and Pytorch.\n",
        "ref_linear.weight.data = torch.from_numpy(student_linear.weight.transpose().astype(np.float32)).clone().detach().requires_grad_(True)\n",
        "ref_linear.bias.data = torch.from_numpy(student_linear.bias.transpose().astype(np.float32)).clone().detach().requires_grad_(True)\n",
        "# check output\n",
        "ref_out = ref_linear(linear_data_pt)\n",
        "student_out = student_linear.forward(linear_data_numpy)\n",
        "print_shape_match(ref_out.data.numpy(), student_out)\n",
        "print_res_match(ref_out.data.numpy(), student_out)\n",
        "# check gradients\n",
        "grad_out, grad_w, grad_b = student_linear.backward(np.ones_like(student_out))\n",
        "ref_out.backward(torch.ones_like(ref_out))\n",
        "print_res_match(linear_data_pt.grad.data.numpy(), grad_out)\n",
        "print_res_match(ref_linear.weight.grad.data.numpy().transpose(), grad_w)\n",
        "print_res_match(ref_linear.bias.grad.data.numpy().transpose(), grad_b)\n",
        "print('\\n')\n",
        "\n",
        "print('Testing MaxPool2d:')\n",
        "# initialize Linear\n",
        "student_maxpool = student.MaxPool2d(kernel_size=2, stride=2)\n",
        "ref_maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "# align input data\n",
        "maxpool_data_pt = Variable(torch.rand(1, 3, 32, 32), requires_grad = True)\n",
        "maxpool_data_numpy = maxpool_data_pt.data.numpy()\n",
        "# check output\n",
        "ref_out = ref_maxpool(maxpool_data_pt)\n",
        "student_out = student_maxpool.forward(maxpool_data_numpy)\n",
        "print_shape_match(ref_out.data.numpy(), student_out)\n",
        "print_res_match(ref_out.data.numpy(), student_out)\n",
        "# check gradients\n",
        "grad_out = student_maxpool.backward(np.ones_like(student_out))\n",
        "ref_out.backward(torch.ones_like(ref_out))\n",
        "print_res_match(maxpool_data_pt.grad.data.numpy(), grad_out)\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SPIu4t1GrXyp"
      },
      "outputs": [],
      "source": [
        "print('Testing ReLU:')\n",
        "# initialize Relu\n",
        "student_relu = student.ReLU()\n",
        "ref_relu = nn.ReLU()\n",
        "# align input data\n",
        "relu_data_pt = Variable(torch.rand(1, 3, 32, 32), requires_grad = True)\n",
        "relu_data_numpy = relu_data_pt.data.numpy()\n",
        "# check output\n",
        "ref_out = ref_relu(relu_data_pt)\n",
        "student_out = student_relu.forward(relu_data_numpy)\n",
        "print_shape_match(ref_out.data.numpy(), student_out)\n",
        "print_res_match(ref_out.data.numpy(), student_out)\n",
        "# check gradients\n",
        "grad_out = student_relu.backward(np.ones_like(student_out))\n",
        "ref_out.backward(torch.ones_like(ref_out))\n",
        "print_res_match(relu_data_pt.grad.data.numpy(), grad_out)\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lpLH6Lyvrd7E"
      },
      "outputs": [],
      "source": [
        "print('Testing BatchNorm1d:')\n",
        "# initialize BN\n",
        "student_bn = student.BatchNorm1d(100)\n",
        "ref_bn = nn.BatchNorm1d(100)\n",
        "# align input data\n",
        "bn_data_pt = Variable(torch.rand(20, 100), requires_grad = True)\n",
        "bn_data_numpy = bn_data_pt.data.numpy()\n",
        "# align BN params\n",
        "ref_bn.weight.data = torch.from_numpy(student_bn.gamma).clone().detach().requires_grad_(True)\n",
        "ref_bn.bias.data = torch.from_numpy(student_bn.beta).clone().detach().requires_grad_(True)\n",
        "ref_bn.running_mean.data = torch.from_numpy(student_bn.r_mean).clone().detach().requires_grad_(False)\n",
        "ref_bn.running_var.data = torch.from_numpy(student_bn.r_var).clone().detach().requires_grad_(False)\n",
        "ref_bn.momentum = student_bn.momentum\n",
        "ref_bn.eps = student_bn.eps\n",
        "# check output\n",
        "ref_out = ref_bn(bn_data_pt)\n",
        "student_out = student_bn.forward(bn_data_numpy, train=True)\n",
        "print_shape_match(ref_out.data.numpy(), student_out)\n",
        "print_res_match(ref_out.data.numpy(), student_out)\n",
        "# check gradients\n",
        "grad_out, grad_gamma, grad_beta = student_bn.backward(np.ones_like(student_out))\n",
        "ref_out.backward(torch.ones_like(ref_out))\n",
        "print_res_match(bn_data_pt.grad.data.numpy(), grad_out)\n",
        "print_res_match(ref_bn.weight.grad.data.numpy(), grad_gamma)\n",
        "print_res_match(ref_bn.bias.grad.data.numpy(), grad_beta)\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qZbQTzVHrgkA"
      },
      "outputs": [],
      "source": [
        "print('Testing cross_entropy_loss_with_softmax:')\n",
        "# initialize cross_entropy loss\n",
        "# Note: PyNet's cross_entropy loss functions as Pytorch\n",
        "# CrossEntropyLoss when reduction='none', So the output loss shape as well\n",
        "# as the gradients shape are N*C shape.\n",
        "student_loss = student.CrossEntropyLossWithSoftmax()\n",
        "ref_loss = nn.CrossEntropyLoss(reduction='none')\n",
        "# align input data\n",
        "loss_data_pt = torch.randn(3, 5, requires_grad = True)\n",
        "loss_data_numpy = loss_data_pt.data.numpy()\n",
        "# align target data\n",
        "loss_target_pt = torch.empty(3, dtype=torch.long).random_(5)\n",
        "loss_target_numpy = loss_target_pt.data.numpy()\n",
        "# check output\n",
        "ref_out = ref_loss(loss_data_pt, loss_target_pt)\n",
        "student_out = student_loss.forward(loss_data_numpy, loss_target_numpy)\n",
        "print_shape_match(ref_out.data.numpy(), student_out)\n",
        "print_res_match(ref_out.data.numpy(), student_out)\n",
        "# check gradients\n",
        "grad_out = student_loss.backward(np.ones_like(student_out))\n",
        "ref_out.backward(torch.ones_like(ref_out))\n",
        "print_res_match(loss_data_pt.grad.data.numpy(), grad_out)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "gradient_check.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
