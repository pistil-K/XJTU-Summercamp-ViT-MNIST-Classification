import pylayer as L

'''
    utility: pointwise two "2-d" lists with weight
    using operator op
    a = a * wa op b * wb
'''
def list2d_add(a, wa, b, wb):
    for x, y in zip(a, b):
        # print(len(x), len(y))
        for u, v in zip(x, y):
            # print('\t', u.shape, v.shape)
            if u.shape != v.shape:
                print(f"Shape mismatch in add: {u.shape} vs {v.shape}")
            u *= wa
            u += v * wb

def list2d_sub(a, wa, b, wb):
    for x, y in zip(a, b):
        # print(len(x), len(y))
        for u, v in zip(x, y):
            # print('\t', u.shape, v.shape)
            if u.shape != v.shape:
                print(f"Shape mismatch in sub: {u.shape} vs {v.shape}")
            u *= wa
            u -= v * wb

class SGD(object):
    '''
        Implements stochastic gradient descent, with momentum features.

        The model could be sequential, must have forward(), backward();
        model.param_grads is generated by model.backward().
    '''
    def __init__(self, model, lr=1e-3, momentum=0.):
        self.lr = lr
        self.momentum = momentum
        self.last_step_grads = None
        self.model = model
        # maintains a reference of all trainable params of all layers
        self.params_ref = []
        for l in self.model.layers:
            # Get layer type name
            layer_type = l.__class__.__name__
            
            # Handle different layer types
            if layer_type == 'Linear':
                self.params_ref.append((l.weight, l.bias))
            elif layer_type in ['BatchNorm1d', 'BatchNorm2d']:
                self.params_ref.append((l.gamma, l.beta))
            elif layer_type in ['ReLU', 'CrossEntropyLossWithSoftmax', 'MaxPool2d', 'Flatten', 'GlobalAveragePooling']:
                self.params_ref.append(())
            elif layer_type == 'Conv2d':
                self.params_ref.append((l.weight, l.bias))
            elif layer_type == 'PatchEmbedding':
                self.params_ref.append((l.proj.weight, l.proj.bias))
            elif layer_type == 'TransformerBlock':
                # 收集Transformer块中所有可训练参数
                params = []
                # Multi-head attention参数
                params.extend([
                    l.attn.q_proj.weight, l.attn.q_proj.bias,
                    l.attn.k_proj.weight, l.attn.k_proj.bias,
                    l.attn.v_proj.weight, l.attn.v_proj.bias,
                    l.attn.out_proj.weight, l.attn.out_proj.bias
                ])
                # Batch normalization参数
                params.extend([l.norm1.gamma, l.norm1.beta])
                params.extend([l.norm2.gamma, l.norm2.beta])
                # MLP参数
                params.extend([
                    l.mlp.fc1.weight, l.mlp.fc1.bias,
                    l.mlp.fc2.weight, l.mlp.fc2.bias
                ])
                self.params_ref.append(tuple(params))
            else:
                raise ValueError(f"Unknown layer type: {layer_type}")

    def step(self):
        if (self.momentum > 0. and self.last_step_grads != None):
            list2d_add(self.model.param_grads, 1. - self.momentum,
                        self.last_step_grads, self.momentum)
        list2d_sub(self.params_ref, 1., self.model.param_grads, self.lr)
        self.last_step_grads = self.model.param_grads.copy()
